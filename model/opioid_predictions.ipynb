{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opioid_predictions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUOGxK5aYPNb",
        "colab_type": "code",
        "outputId": "727eb5ab-9bff-4b86-9d47-640441cecb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "!pip install --upgrade scikit-learn==0.20.2\n",
        "!pip install shap\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import pickle\n",
        "# %load /content/drive/My Drive/opioid_functions.py\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# open a connection\n",
        "conn = sqlite3.connect('/content/drive/My Drive/DVADB/DVADB.db')\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scikit-learn==0.20.2 in /usr/local/lib/python3.6/dist-packages (0.20.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.2) (1.17.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.2) (1.3.1)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.6/dist-packages (0.31.0)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (0.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.20.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->shap) (1.12.0)\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeusEwnTjyFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier, RandomForestRegressor\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import RFECV\n",
        "import shap\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "def clean_credentials(df):\n",
        "\n",
        "    # MD\n",
        "    df['credential_md'] = np.where(\n",
        "        df['nppes_credentials'].str.replace(\".\", \"\").str.contains(\"MD\"),\n",
        "        1,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # RN\n",
        "    df['credential_rn'] = np.where(\n",
        "        df['nppes_credentials'].str.replace(\".\", \"\").str.contains(\"RN\"),\n",
        "        1,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # PHD\n",
        "    df['credential_phd'] = np.where(\n",
        "        df['nppes_credentials'].str.replace(\".\", \"\").str.contains(\"PHD\"),\n",
        "        1,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # DDS\n",
        "    df['credential_dds'] = np.where(\n",
        "        df['nppes_credentials'].str.replace(\".\", \"\").str.contains(\"DDS\"),\n",
        "        1,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # PA\n",
        "    df['credential_pa'] = np.where(\n",
        "        df['nppes_credentials'].str.replace(\".\", \"\").str.contains(\"PA\"),\n",
        "        1,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # MBA\n",
        "    df['credential_mba'] = np.where(\n",
        "        df['nppes_credentials'].str.replace(\".\", \"\").str.contains(\"MBA\"),\n",
        "        1,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def ks_distribution_test(df_a, df_b, alpha=.05):\n",
        "    dist_test = stats.ks_2samp(df_a, df_b)\n",
        "\n",
        "    if dist_test[1] < alpha:\n",
        "        print(\n",
        "            df_a.name,\n",
        "            ': retained and cancelled distributions are different'\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        print(\n",
        "            df_b.name,\n",
        "            ': retained and cancelled distributions are the same'\n",
        "        )\n",
        "\n",
        "\n",
        "def dummy_wrapper(df, cols_to_dummy=None):\n",
        "    \"\"\"\n",
        "    Wrapper for pd.get_dummies that appends dummy variables back onto original dataset, cleans columns\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "    cols_to_dummy : list\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    df_dummy = pd.get_dummies(df[cols_to_dummy], dummy_na=True)\n",
        "\n",
        "    # clean the categorical column names\n",
        "    df_dummy.columns = df_dummy.columns. \\\n",
        "        str.strip(). \\\n",
        "        str.lower(). \\\n",
        "        str.replace(' ', '_'). \\\n",
        "        str.replace('-', ''). \\\n",
        "        str.replace('/', ''). \\\n",
        "        str.replace('$', ''). \\\n",
        "        str.replace(',', ''). \\\n",
        "        str.replace('&', ''). \\\n",
        "        str.replace('.', ''). \\\n",
        "        str.replace('+', ''). \\\n",
        "        str.replace(':', ''). \\\n",
        "        str.replace('|', ''). \\\n",
        "        str.replace('[', ''). \\\n",
        "        str.replace(']', ''). \\\n",
        "        str.replace('(', '').str.replace(')', '')\n",
        "\n",
        "    cols_to_keep = [c for c in df.columns if c not in cols_to_dummy]\n",
        "\n",
        "    df_keep = df[cols_to_keep]\n",
        "\n",
        "    df_clean = pd.concat([df_keep, df_dummy], axis=1)\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "\n",
        "def variance_threshold(df, threshold=.001):\n",
        "    \"\"\"\n",
        "    Checks model frame for numeric columns with zero variance and variance\n",
        "    less than a provided threshold\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "    threshold: float\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"number of features before filter {df.shape[1]}\")\n",
        "\n",
        "    var_dict = np.var(df, axis=0).to_dict()\n",
        "\n",
        "    var_dict = {k: v for (k, v) in var_dict.items() if v >= threshold}\n",
        "\n",
        "    keep_list = list(var_dict.keys())\n",
        "\n",
        "    df = df[keep_list]\n",
        "\n",
        "    print(f\"number of features remaining {df.shape[1]}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_categorical(df):\n",
        "    \"\"\"\n",
        "    Simple parser to exclude random categorical column characters from the final value\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.str.strip(). \\\n",
        "        str.lower(). \\\n",
        "        str.replace(' ', '_'). \\\n",
        "        str.replace('-', ''). \\\n",
        "        str.replace('/', ''). \\\n",
        "        str.replace('$', ''). \\\n",
        "        str.replace(',', ''). \\\n",
        "        str.replace('&', ''). \\\n",
        "        str.replace('.', ''). \\\n",
        "        str.replace('|', ''). \\\n",
        "        str.replace('(', '').str.replace(')', '')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def extra_trees_vimp(\n",
        "        df,\n",
        "        y,\n",
        "        threshold=.01,\n",
        "        plot=True,\n",
        "        estimators=100,\n",
        "        depth=3,\n",
        "        split_sample=.05,\n",
        "        leaf_sample=.05,\n",
        "        transform=True\n",
        "):\n",
        "\n",
        "    print('Building Trees...')\n",
        "\n",
        "    x_vars = df\n",
        "    y_vars = y\n",
        "\n",
        "    # flow control for regression or classification\n",
        "    regression_type = y_vars.drop_duplicates()\n",
        "\n",
        "    if len(regression_type) == 2:\n",
        "\n",
        "        print('Building Classification Trees...')\n",
        "\n",
        "        model = ExtraTreesClassifier(\n",
        "            n_estimators=estimators,\n",
        "            max_depth=depth,\n",
        "            random_state=444,\n",
        "            min_samples_split=split_sample,\n",
        "            min_samples_leaf=leaf_sample,\n",
        "            class_weight='balanced_subsample',\n",
        "            max_features='log2',\n",
        "            bootstrap=True,\n",
        "            oob_score=True\n",
        "            )\n",
        "\n",
        "        model.fit(x_vars, np.asarray(y_vars).ravel())\n",
        "\n",
        "        importance = model.feature_importances_\n",
        "\n",
        "        df = pd.DataFrame(importance)\n",
        "        df = df.T\n",
        "        df.columns = x_vars.columns\n",
        "        df = df.T.reset_index()\n",
        "        df.columns = ['variable', 'tree_vimp']\n",
        "        df = df.sort_values('tree_vimp', ascending=False)\n",
        "\n",
        "    else:\n",
        "\n",
        "        if transform:\n",
        "            y_vars = np.sqrt(y_vars)\n",
        "\n",
        "        print('Building Regression Trees...')\n",
        "\n",
        "        model = ExtraTreesRegressor(\n",
        "            n_estimators=estimators,\n",
        "            max_depth=depth,\n",
        "            random_state=444,\n",
        "            min_samples_split=split_sample,\n",
        "            min_samples_leaf=leaf_sample,\n",
        "            max_features='log2',\n",
        "            bootstrap=True,\n",
        "            oob_score=True\n",
        "            )\n",
        "        model.fit(x_vars, np.asarray(y_vars).ravel())\n",
        "\n",
        "        importance = model.feature_importances_\n",
        "\n",
        "        df = pd.DataFrame(importance)\n",
        "        df = df.T\n",
        "        df.columns = x_vars.columns\n",
        "        df = df.T.reset_index()\n",
        "        df.columns = ['variable', 'tree_vimp']\n",
        "        df = df.sort_values('tree_vimp', ascending=False)\n",
        "\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "        sns.barplot(\n",
        "            x='tree_vimp',\n",
        "            y='variable',\n",
        "            data=df[df.tree_vimp >= threshold],\n",
        "            palette='Blues_r',\n",
        "        ).set_title(y.name)\n",
        "\n",
        "    # extract the best tree importance results\n",
        "    df = df[df.tree_vimp >= threshold]\n",
        "    important_cols = list(df.variable)\n",
        "\n",
        "    print('Tree Models Complete')\n",
        "\n",
        "    return df, important_cols, model.oob_score_\n",
        "\n",
        "\n",
        "def clean_multi_index_headers(df):\n",
        "    \"\"\"\n",
        "    Concatenates a multi-index columns headers into one with a clean format\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# set up the plot decision tree function\n",
        "# def plot_cart(\n",
        "#         estimator,\n",
        "#         file_name='my_tree.dot',\n",
        "#         feature_name='metric1',\n",
        "#         class_name=('LOST', 'WON')\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Visualizes a CART decision tree using graphviz plotting\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     estimator : fitted decision tree object\n",
        "#     file_name : filename to store the graphviz plot\n",
        "#     feature_name : name of features available in decision tree\n",
        "#     class_name : label of the classifier \"classes\" the decision tree solves for\n",
        "#     Returns\n",
        "#     -------\n",
        "#     graphviz object\n",
        "#     \"\"\"\n",
        "#\n",
        "#     tree.export_graphviz(\n",
        "#         estimator,\n",
        "#         out_file=file_name,\n",
        "#         filled=True,\n",
        "#         special_characters=True,\n",
        "#         rounded=True,\n",
        "#         feature_names=feature_name,\n",
        "#         class_names=class_name,\n",
        "#         proportion=True,\n",
        "#         rotate=False,\n",
        "#         precision=3\n",
        "#     )\n",
        "#\n",
        "#     with open(file_name) as f:\n",
        "#         dot_graph = f.read()\n",
        "#\n",
        "#     return graphviz.Source(dot_graph)\n",
        "\n",
        "\n",
        "def extract_vimp(clf, column_names, threshold=.001):\n",
        "    \"\"\"\n",
        "    Helper function to extract variable importance from the feature importance based model\n",
        "    Takes in a fit object and extract model feature importance into a dataframe\n",
        "    Need to supply column headers\n",
        "    Parameters\n",
        "    ----------\n",
        "    clf : model.fit() object\n",
        "    column_names : dataframe of features\n",
        "    threshold : limit returned values to by vimp threshold\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    importance = clf.feature_importances_\n",
        "\n",
        "    df = pd.DataFrame(importance)\n",
        "    df = df.T\n",
        "    df.columns = column_names.columns\n",
        "    df = df.T.reset_index()\n",
        "    df.columns = ['variable', 'tree_vimp']\n",
        "    df = df.sort_values('tree_vimp', ascending=False)\n",
        "\n",
        "    # extract the best tree importance results\n",
        "    df = df[df.tree_vimp >= threshold]\n",
        "    important_cols = list(df.variable)\n",
        "\n",
        "    return df, important_cols\n",
        "\n",
        "\n",
        "def prune_index(inner_tree, index, threshold):\n",
        "    if inner_tree.value[index].min() < threshold:\n",
        "        # turn node into a leaf by \"un linking\" its children\n",
        "        inner_tree.children_left[index] = TREE_LEAF\n",
        "        inner_tree.children_right[index] = TREE_LEAF\n",
        "\n",
        "    # if there are children, visit them as well\n",
        "    if inner_tree.children_left[index] != TREE_LEAF:\n",
        "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
        "        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n",
        "\n",
        "\n",
        "def get_difference(data, interval=1):\n",
        "    diff = list()\n",
        "    for i in range(interval, len(data)):\n",
        "        value = data[i] - data[i - interval]\n",
        "        diff.append(value)\n",
        "    return pd.Series(diff)\n",
        "\n",
        "\n",
        "def tree_weight_cv(target, features, weight_max=20):\n",
        "    \"\"\"\n",
        "    Quick cross validation for class weights for a decision tree classifier\n",
        "    Parameters\n",
        "    ----------\n",
        "    target : target values to fit as a classifier\n",
        "    features : frame of features to fit in model\n",
        "    weight_max : range max to cross validate through\n",
        "    Returns\n",
        "    -------\n",
        "    set\n",
        "    \"\"\"\n",
        "\n",
        "    weight_dict = dict()\n",
        "\n",
        "    for i in range(0, weight_max, 1):\n",
        "        print(i)\n",
        "\n",
        "        # initialize the CART\n",
        "        mvc_tree = DecisionTreeClassifier(\n",
        "            criterion='entropy',\n",
        "            splitter='best',\n",
        "            max_leaf_nodes=300,\n",
        "            random_state=478946,\n",
        "            # class_weight='balanced',\n",
        "            class_weight={1: i, 0: 1},\n",
        "            min_samples_leaf=.0001,\n",
        "            min_samples_split=.0001\n",
        "        )\n",
        "\n",
        "        # fit the decision tree\n",
        "        estimator: object = mvc_tree.fit(\n",
        "            X=features,\n",
        "            y=target\n",
        "        )\n",
        "\n",
        "        # get the cross validation score for fitted model\n",
        "        cv_tree = cross_validate(\n",
        "            estimator,\n",
        "            features,\n",
        "            target,\n",
        "            cv=10,\n",
        "            scoring='roc_auc',\n",
        "            return_estimator=True,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        print(np.mean(cv_tree['test_score']))\n",
        "        print(np.mean(estimator.predict(features)))\n",
        "\n",
        "        weight_dict[i] = np.mean(cv_tree['test_score'])\n",
        "\n",
        "    return {weight_dict, estimator, cv_tree}\n",
        "\n",
        "\n",
        "def scale_variables(train, test):\n",
        "    \"\"\"\n",
        "    Applies StandardScalar() learned on the training dataset to the training and test data frames\n",
        "    Applying the learned scalar on train to the test frames helps avoid information leakage\n",
        "    Parameters\n",
        "    ----------\n",
        "    train : training data frame of features\n",
        "    test : testing data frame of features\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame : scaled training and test set data frames\n",
        "    fit object: StandardScaler() fit object to use for future predictions\n",
        "    \"\"\"\n",
        "    selector = StandardScaler()\n",
        "\n",
        "    return (\n",
        "        pd.DataFrame(selector.fit_transform(train), columns=train.columns, index=train.index),\n",
        "        pd.DataFrame(selector.transform(test), columns=train.columns),\n",
        "        selector.fit(train)\n",
        "    )\n",
        "\n",
        "\n",
        "def feature_contributions(model, df, plot_results=False):\n",
        "    \"\"\"\n",
        "    Extract the shapley contributions for a tree based model\n",
        "    Place results into a frame that can be joined back onto original frame\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : fitted tree based model\n",
        "    df : frame of training or test data\n",
        "    plot_results : indicator to plot summary of feature contributions\n",
        "    Returns\n",
        "    -------\n",
        "    set\n",
        "    \"\"\"\n",
        "\n",
        "    explain_tree = shap.TreeExplainer(model)\n",
        "    shp_values = explain_tree.shap_values(df)\n",
        "\n",
        "    if plot_results:\n",
        "        plt.figure(figsize=(10, 20))\n",
        "        shap.summary_plot(shp_values, df)\n",
        "        plt.show()\n",
        "\n",
        "    return pd.DataFrame(shp_values, columns=df.columns, index=df.index), explain_tree, shp_values\n",
        "\n",
        "\n",
        "def shp_outputs(model, x_shp_, y_shp_, plot_results=True):\n",
        "    \"\"\"\n",
        "    Extract the shapley contributions for a tree based model\n",
        "    Save features in a full data frame\n",
        "    Plot features in barplot for global contribution by feature\n",
        "    Plot features in a heatmap for local contribution by training / test sample\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : fitted tree based model from sklearn\n",
        "    x_shp_ : frame of training or testing observations\n",
        "    y_shp_ : frame of training or testing responses\n",
        "    plot_results : indicator to plot the barplot and heatmap with seaborn\n",
        "    Returns\n",
        "    -------\n",
        "    set\n",
        "    \"\"\"\n",
        "    fc_, shp_explain, shp_values = feature_contributions(model, x_shp_)\n",
        "    fc_ = pd.concat(\n",
        "        [fc_.reset_index(), pd.Series(y_shp_).rename('index')],\n",
        "        axis=1,\n",
        "        ignore_index=False\n",
        "    ).drop('index', axis=1)\n",
        "\n",
        "    # group all features by their average shp contribution\n",
        "    fc_summary = pd.DataFrame(fc_.mean()).reset_index()\n",
        "    fc_summary.columns = ['feature', 'avg_contribution']\n",
        "    fc_summary['contribution_type'] = np.where(\n",
        "        fc_summary.avg_contribution < 0,\n",
        "        'negative',\n",
        "        'positive'\n",
        "    )\n",
        "\n",
        "    if plot_results:\n",
        "        plt.rcParams['figure.figsize'] = (14, 16)\n",
        "        sns.barplot(\n",
        "            x='avg_contribution',\n",
        "            y='feature',\n",
        "            data=fc_summary[abs(fc_summary.avg_contribution) > 0.0],\n",
        "            hue='contribution_type',\n",
        "            palette=['#FF0D57', '#1E88E5']\n",
        "        ).set_title('Average Shapley Contribution by Feature')\n",
        "        plt.figure()\n",
        "        plt.show()\n",
        "\n",
        "        # subset out random columns\n",
        "        keep_cols = fc_.iloc[:, :-1].columns.to_series().sample(frac=.20)\n",
        "\n",
        "        plt.rcParams['figure.figsize'] = (14, 6)\n",
        "        sns.heatmap(\n",
        "            fc_[keep_cols].sample(50).T,\n",
        "            center=np.mean(fc_.values),\n",
        "            robust=True,\n",
        "            cbar=False,\n",
        "            square=True,\n",
        "            xticklabels=False,\n",
        "            cmap=['#FF0D57', '#FFC3D5', 'whitesmoke', '#1E88E5', '#D1E6FA']\n",
        "        ).set_title('Variable Contribution: Churn Model')\n",
        "        plt.figure()\n",
        "        plt.show()\n",
        "\n",
        "    return fc_, shp_explain, shp_values, fc_summary\n",
        "\n",
        "\n",
        "def feature_extraction_forest(\n",
        "        df=None,\n",
        "        features=None,\n",
        "        target=None,\n",
        "        folds=5,\n",
        "        step_size=.01,\n",
        "        model_type='classifier',\n",
        "        select_n=2\n",
        "):\n",
        "    \"\"\"\n",
        "    Recursive Feature Extraction wrapper\n",
        "    Run RFE with model type \"classifier\" or \"regression\" depending on target\n",
        "    Return the top features selected from the RFE process\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : data frame of training and test responses\n",
        "    features : frame of features\n",
        "    target : frame of targets\n",
        "    folds : number of cross validation folds for the RFE estimator to be tested against\n",
        "    step_size : percent of features to eliminate at each round, based on variable importance\n",
        "    model_type : indicator letting process know we are in a classification or regression problem\n",
        "    select_n : number of ranked features to keep, 1 indicates the best ranked features set\n",
        "    Returns\n",
        "    -------\n",
        "    set\n",
        "    \"\"\"\n",
        "\n",
        "    if model_type == 'classifier':\n",
        "\n",
        "        # initialize the random forest for feature extraction\n",
        "        rf_estimator = RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            min_samples_leaf=.03,\n",
        "            min_samples_split=.03,\n",
        "            max_features='log2',\n",
        "            bootstrap=True\n",
        "        )\n",
        "\n",
        "        rf_fit = rf_estimator.fit(X=features, y=target)\n",
        "        tree_vars = extract_vimp(rf_fit, column_names=features)\n",
        "\n",
        "        # set up the automated feature extraction model\n",
        "        rfe = RFECV(\n",
        "            estimator=rf_estimator,\n",
        "            cv=folds,\n",
        "            step=step_size,\n",
        "            verbose=3\n",
        "        )\n",
        "\n",
        "        # fit the RFE model\n",
        "        rfe_fit = rfe.fit(X=features, y=target)\n",
        "\n",
        "        # rfe details\n",
        "        ranked_features = list(rfe_fit.ranking_)\n",
        "        best_features = [i for i, f in enumerate(ranked_features) if f <= select_n]\n",
        "        best_cols = list(df.iloc[:, best_features].columns)\n",
        "        best_cols = list(set(best_cols))\n",
        "        print(len(best_cols))\n",
        "\n",
        "        return best_cols, best_features, tree_vars\n",
        "\n",
        "    else:\n",
        "\n",
        "        # initialize the random forest for feature extraction\n",
        "        rf_estimator = RandomForestRegressor(\n",
        "            n_estimators=200,\n",
        "            min_samples_leaf=.03,\n",
        "            min_samples_split=.03,\n",
        "            max_features='log2',\n",
        "            bootstrap=True\n",
        "        )\n",
        "\n",
        "        rf_fit = rf_estimator.fit(X=features, y=target)\n",
        "        tree_vars = extract_vimp(rf_fit, column_names=features)\n",
        "\n",
        "        # set up the automated feature extraction model\n",
        "        rfe = RFECV(\n",
        "            estimator=rf_estimator,\n",
        "            cv=folds,\n",
        "            step=step_size,\n",
        "            verbose=3\n",
        "        )\n",
        "\n",
        "        # fit the RFE model\n",
        "        rfe_fit = rfe.fit(X=features, y=target)\n",
        "\n",
        "        # rfe details\n",
        "        ranked_features = list(rfe_fit.ranking_)\n",
        "        best_features = [i for i, f in enumerate(ranked_features) if f <= select_n]\n",
        "        best_cols = list(df.iloc[:, best_features].columns)\n",
        "        best_cols = list(set(best_cols))\n",
        "        print(len(best_cols))\n",
        "\n",
        "        return best_cols, best_features, tree_vars\n",
        "\n",
        "\n",
        "def plot_performance(gcv):\n",
        "    n_splits = gcv.cv.n_splits\n",
        "    cv_scores = {\"alpha\": [], \"test_score\": [], \"split\": []}\n",
        "    order = []\n",
        "    for i, params in enumerate(gcv.cv_results_[\"params\"]):\n",
        "        name = \"%.5f\" % params[\"alpha\"]\n",
        "        order.append(name)\n",
        "        for j in range(n_splits):\n",
        "            vs = gcv.cv_results_[\"split%d_test_score\" % j][i]\n",
        "            cv_scores[\"alpha\"].append(name)\n",
        "            cv_scores[\"test_score\"].append(vs)\n",
        "            cv_scores[\"split\"].append(j)\n",
        "    df = pd.DataFrame.from_dict(cv_scores)\n",
        "    _, ax = plt.subplots(figsize=(11, 6))\n",
        "    sns.boxplot(x=\"alpha\", y=\"test_score\", data=df, order=order, ax=ax)\n",
        "    _, x_text = plt.xticks()\n",
        "    for t in x_text:\n",
        "        t.set_rotation(\"vertical\")\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuuLht_WhIGg",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnhuxW-mgqGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "de95d64d-88a0-4f73-b0b8-f424c481c0e1"
      },
      "source": [
        "# read from the main table\n",
        "df = pd.read_sql_query(\n",
        "    \"SELECT * FROM npi_summary WHERE nppes_provider_country = 'US'\",\n",
        "    conn\n",
        ")\n",
        "\n",
        "# tuck in npi to expose for joining later\n",
        "df = df.set_index('npi')\n",
        "\n",
        "# load the model\n",
        "loaded_model = pickle.load(open('/content/drive/My Drive/opioid_gbm_full.sav', 'rb'))\n",
        "\n",
        "# import the final features from development model\n",
        "features = list(pd.read_pickle('/content/drive/My Drive/opioid_features.sav'))\n",
        "\n",
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.19.2 when using version 0.20.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator GradientBoostingRegressor from version 0.19.2 when using version 0.20.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1162732, 34)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9S_P_F5X8cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# columns that we cannot use for modeling\n",
        "drop_cols = [\n",
        "    'nppes_provider_last_org_name',\n",
        "    'nppes_provider_first_name',\n",
        "    'nppes_provider_street1',\n",
        "    'nppes_provider_street2',\n",
        "    'opioid_claim_count' # does this feature add leakage?\n",
        "]\n",
        "\n",
        "df.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "# format columns to numeric\n",
        "for i in df.columns:\n",
        "    if df[i].dtype == object:\n",
        "        df[i] = df[i].apply(\n",
        "            lambda x: 0 if x == '' else x\n",
        "        )\n",
        "\n",
        "# clean credentials into usable meta groups\n",
        "df = clean_credentials(df)\n",
        "df.drop('nppes_credentials', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVd0zmb4ptNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['nppes_provider_city'] = np.where(\n",
        "    df['nppes_provider_city'] == 'HOUSTON',\n",
        "    df['nppes_provider_city'],\n",
        "    np.where(\n",
        "        df['nppes_provider_city'] == 'NEW YORK',\n",
        "        df['nppes_provider_city'],\n",
        "        np.where(\n",
        "            df['nppes_provider_city'] == 'CHICAGO',\n",
        "            df['nppes_provider_city'],\n",
        "            'Other'\n",
        "        )\n",
        "    )  \n",
        ")\n",
        "\n",
        "# list of categorical columns\n",
        "cols_to_dummy = [\n",
        "    'nppes_provider_gender', \n",
        "    'nppes_provider_state',\n",
        "    'nppes_provider_country',\n",
        "    'nppes_provider_city',\n",
        "    'specialty_description'\n",
        "]\n",
        "\n",
        "# dummy out the categorical columns\n",
        "df = dummy_wrapper(df, cols_to_dummy)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4AtOrhvqvq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop target from prediction set\n",
        "df.drop(\n",
        "    'opioid_prescriber_rate',\n",
        "    axis=1,\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "# subset to only the needed features\n",
        "df = df[features]\n",
        "\n",
        "# some duplicated columns leak in...\n",
        "df = df.loc[:, ~df.columns.duplicated()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWa0C3rRpsb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict onto the entire dataset \n",
        "gbm_predictions = loaded_model.predict(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez9njAeOYA8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract the shap values\n",
        "shp_df, explain, shp_v = feature_contributions(loaded_model, df)\n",
        "\n",
        "# append predictions onto original frame\n",
        "df['gbm_predict'] = gbm_predictions\n",
        "\n",
        "# append shp values back onto original prediction frame\n",
        "df = df.merge(\n",
        "    shp_df,\n",
        "    how='inner',\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    suffixes=('', '_shp')\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBnWrVckYD0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# expose npi for joining\n",
        "df = df.reset_index()\n",
        "\n",
        "# write final results back into sqlite database\n",
        "if conn is not None:\n",
        "    df.to_sql(\n",
        "        'npi_predictions',\n",
        "        conn,\n",
        "        if_exists='replace'\n",
        "    )\n",
        "\n",
        "df.to_pickle('/content/drive/My Drive/DVADB/npi_predictions.pkl')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIKeROa-7vo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in original for joining\n",
        "df_original = pd.read_sql_query(\n",
        "    \"SELECT * FROM npi_summary WHERE nppes_provider_country = 'US'\",\n",
        "    conn\n",
        ")\n",
        "\n",
        "# combine predictions with original\n",
        "df_combined = df_original.merge(\n",
        "    df,\n",
        "    how='inner',\n",
        "    on='npi'\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOE7QlUAjwMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_combined['opioid_prescriber_rate'] = df_combined['opioid_prescriber_rate'].replace(\"\", 0).astype(float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFIksEB_kYIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_combined.drop(list(df_combined.filter(regex = '_y')), axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "# calculate residuals\n",
        "df_combined['diff'] = df_combined['opioid_prescriber_rate'] - df_combined['gbm_predict']\n",
        "df_combined['abs_diff'] = abs(df_combined['diff'])\n",
        "\n",
        "df_combined.to_pickle('/content/drive/My Drive/DVADB/npi_summary_with_predictions.pkl')\n",
        "\n",
        "# write final results back into sqlite database\n",
        "if conn is not None:\n",
        "    df_combined.to_sql(\n",
        "        'npi_summary_with_predictions',\n",
        "        conn,\n",
        "        if_exists='replace'\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu1QSPDD712n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61a34108-f6e2-453a-98a0-73e345c0860d"
      },
      "source": [
        "# subset to only large residuals\n",
        "df_outliers = df_combined[df_combined['abs_diff'] >= 5]\n",
        "print(df_outliers.shape)\n",
        "\n",
        "# write final results back into sqlite database\n",
        "if conn is not None:\n",
        "    df_outliers.to_sql(\n",
        "        \"npi_outliers_only\",\n",
        "        conn,\n",
        "        if_exists=\"replace\"\n",
        "    )\n",
        "\n",
        "df_outliers.to_pickle('/content/drive/My Drive/DVADB/npi_outliers.pkl')\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101647, 176)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}